x1000c1s5b0n0.hostmgmt2000.cm.asp2a.nscc.sg
x1000c2s0b0n1.hostmgmt2000.cm.asp2a.nscc.sg
Fri Oct 11 10:56:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:03:00.0 Off |                    0 |
| N/A   43C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:41:00.0 Off |                    0 |
| N/A   43C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:81:00.0 Off |                    0 |
| N/A   42C    P0              60W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:C1:00.0 Off |                    0 |
| N/A   42C    P0              59W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
mpirun -wdir /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama -output-filename /home/users/industry/ai-hpc/apacsc22/run/output/llama.nodes2.GBS128.MBS32.8335473.pbs101 -map-by ppr:4:node -oversubscribe -report-bindings -x NCCL_DEBUG=INFO -x NCCL_IB_DISABLE=1 -mca pml ^ucx -x NCCL_NET_GDR_LEVEL=0 /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/bin/litgpt finetune_full /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf --out_dir /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full --data JSON --data.json_path /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024 --config /home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/full.yaml --eval.final_validation=false --train.epochs=1 --devices=4 --num_nodes=2 --train.max_steps=20 --train.global_batch_size=128 --train.micro_batch_size=32
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14968aa8fc20>,
              ignore_index=-100,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1492ae95e630>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14d7a90bd880>,
              ignore_index=-100,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1545d3123620>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14ac13e51c70>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1515c409d700>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14c956188470>,
              ignore_index=-100,
              seed=42,
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1492c0681c70>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c1s5b0n0:1384743] MCW rank 0 is not bound (or bound to all available processors)
[x1000c1s5b0n0:1384746] MCW rank 3 is not bound (or bound to all available processors)
[x1000c2s0b0n1:309941] MCW rank 5 is not bound (or bound to all available processors)
[x1000c1s5b0n0:1384745] MCW rank 2 is not bound (or bound to all available processors)
[x1000c1s5b0n0:1384744] MCW rank 1 is not bound (or bound to all available processors)
[x1000c2s0b0n1:309942] MCW rank 6 is not bound (or bound to all available processors)
[x1000c2s0b0n1:309943] MCW rank 7 is not bound (or bound to all available processors)
[x1000c2s0b0n1:309940] MCW rank 4 is not bound (or bound to all available processors)
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           x1000c1s5b0n0
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
All GPUs are fully connected via NVLink.
All GPUs are fully connected via NVLink.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Number of trainable parameters: 6,738,415,616
Number of trainable parameters: 6,738,415,616
[x1000c1s5b0n0:1384730] 15 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1000c1s5b0n0:1384730] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.117<0>
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
x1000c1s5b0n0:1384744:1384744 [1] NCCL INFO cudaDriverVersion 12020
x1000c1s5b0n0:1384746:1384746 [3] NCCL INFO cudaDriverVersion 12020
x1000c1s5b0n0:1384745:1384745 [2] NCCL INFO cudaDriverVersion 12020
x1000c1s5b0n0:1384744:1384744 [1] NCCL INFO Bootstrap : Using hsn0:10.150.0.117<0>
x1000c1s5b0n0:1384746:1384746 [3] NCCL INFO Bootstrap : Using hsn0:10.150.0.117<0>
x1000c1s5b0n0:1384745:1384745 [2] NCCL INFO Bootstrap : Using hsn0:10.150.0.117<0>
x1000c1s5b0n0:1384746:1384746 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c1s5b0n0:1384744:1384744 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c1s5b0n0:1384745:1384745 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s0b0n1:309942:309942 [2] NCCL INFO cudaDriverVersion 12020
x1000c2s0b0n1:309943:309943 [3] NCCL INFO cudaDriverVersion 12020
x1000c2s0b0n1:309941:309941 [1] NCCL INFO cudaDriverVersion 12020
x1000c2s0b0n1:309940:309940 [0] NCCL INFO cudaDriverVersion 12020
x1000c2s0b0n1:309943:309943 [3] NCCL INFO Bootstrap : Using hsn0:10.150.0.156<0>
x1000c2s0b0n1:309941:309941 [1] NCCL INFO Bootstrap : Using hsn0:10.150.0.156<0>
x1000c2s0b0n1:309942:309942 [2] NCCL INFO Bootstrap : Using hsn0:10.150.0.156<0>
x1000c2s0b0n1:309940:309940 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.156<0>
x1000c2s0b0n1:309942:309942 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s0b0n1:309943:309943 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s0b0n1:309940:309940 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s0b0n1:309941:309941 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.117<0> [1]hsn1:10.150.0.244<0> [2]bond0:10.168.0.35<0>
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Using non-device net plugin version 0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Using network Socket
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.117<0> [1]hsn1:10.150.0.244<0> [2]bond0:10.168.0.35<0>
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Using non-device net plugin version 0
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Using network Socket
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.117<0> [1]hsn1:10.150.0.244<0> [2]bond0:10.168.0.35<0>
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Using non-device net plugin version 0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Using network Socket
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.117<0> [1]hsn1:10.150.0.244<0> [2]bond0:10.168.0.35<0>
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Using non-device net plugin version 0
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Using network Socket
x1000c2s0b0n1:309942:311302 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s0b0n1:309942:311302 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.156<0> [1]hsn1:10.150.0.160<0> [2]bond0:10.168.0.20<0>
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Using non-device net plugin version 0
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Using network Socket
x1000c2s0b0n1:309941:311300 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s0b0n1:309943:311301 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.156<0> [1]hsn1:10.150.0.160<0> [2]bond0:10.168.0.20<0>
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Using non-device net plugin version 0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Using network Socket
x1000c2s0b0n1:309941:311300 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.156<0> [1]hsn1:10.150.0.160<0> [2]bond0:10.168.0.20<0>
x1000c2s0b0n1:309943:311301 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.156<0> [1]hsn1:10.150.0.160<0> [2]bond0:10.168.0.20<0>
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Using non-device net plugin version 0
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Using network Socket
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Using non-device net plugin version 0
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Using network Socket
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO comm 0x14511ca0 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x59d1f031ecde0ebc - Init START
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO comm 0x1487eea0 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x59d1f031ecde0ebc - Init START
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init START
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO comm 0x14e38430 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x59d1f031ecde0ebc - Init START
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init START
x1000c2s0b0n1:309942:311302 [2] NCCL INFO comm 0x13c9a0f0 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x59d1f031ecde0ebc - Init START
x1000c2s0b0n1:309941:311300 [1] NCCL INFO comm 0x148d8f00 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x59d1f031ecde0ebc - Init START
x1000c2s0b0n1:309943:311301 [3] NCCL INFO comm 0x1526c2c0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x59d1f031ecde0ebc - Init START
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c2s0b0n1:309941:311300 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c2s0b0n1:309943:311301 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c2s0b0n1:309942:311302 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO comm 0x1487eea0 rank 3 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO P2P Chunksize set to 131072
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO comm 0x14e38430 rank 2 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO P2P Chunksize set to 131072
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/04 :    0   3   6   5   4   7   2   1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/04 :    0   5   7   6   4   1   3   2
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/04 :    0   3   6   5   4   7   2   1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/04 :    0   5   7   6   4   1   3   2
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO P2P Chunksize set to 131072
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO comm 0x14511ca0 rank 1 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309941:311300 [1] NCCL INFO comm 0x148d8f00 rank 5 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
x1000c2s0b0n1:309941:311300 [1] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309943:311301 [3] NCCL INFO comm 0x1526c2c0 rank 7 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
x1000c2s0b0n1:309943:311301 [3] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309942:311302 [2] NCCL INFO comm 0x13c9a0f0 rank 6 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
x1000c2s0b0n1:309942:311302 [2] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/-1/-1->4->0 [2] 5/0/-1->4->-1 [3] 5/0/-1->4->-1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 01/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 03/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 03/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 01/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 01/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 03/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 03/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 01/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 01/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 03/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 03/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 00/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 02/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 00/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 00/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 02/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 02/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 00/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 02/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Connected all rings
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Connected all rings
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Connected all rings
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Connected all rings
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Connected all rings
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Connected all rings
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Connected all rings
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Connected all rings
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 00/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 00/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 00/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 01/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 01/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 01/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 02/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 02/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 02/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 03/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 03/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 03/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 00/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 02/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Channel 02/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 03/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 00/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Channel 02/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 01/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 01/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 01/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Channel 03/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Channel 03/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Channel 03/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Channel 03/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c2s0b0n1:309943:311301 [3] NCCL INFO Connected all trees
x1000c2s0b0n1:309943:311301 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s0b0n1:309943:311301 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO Connected all trees
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO Connected all trees
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s0b0n1:309942:311302 [2] NCCL INFO Connected all trees
x1000c2s0b0n1:309942:311302 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Connected all trees
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s0b0n1:309942:311302 [2] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO Connected all trees
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s0b0n1:309941:311300 [1] NCCL INFO Connected all trees
x1000c2s0b0n1:309941:311300 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s0b0n1:309941:311300 [1] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Connected all trees
x1000c2s0b0n1:309940:311299 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s0b0n1:309940:311299 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c1s5b0n0:1384746:1386096 [3] NCCL INFO comm 0x1487eea0 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c1s5b0n0:1384745:1386098 [2] NCCL INFO comm 0x14e38430 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c1s5b0n0:1384744:1386097 [1] NCCL INFO comm 0x14511ca0 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c2s0b0n1:309942:311302 [2] NCCL INFO comm 0x13c9a0f0 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c2s0b0n1:309941:311300 [1] NCCL INFO comm 0x148d8f00 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x59d1f031ecde0ebc - Init COMPLETE
x1000c2s0b0n1:309943:311301 [3] NCCL INFO comm 0x1526c2c0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x59d1f031ecde0ebc - Init COMPLETE
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
Epoch 1 | iter 1 step 1 | loss train: 1.417, val: n/a | iter time: 10486.38 ms (step)
Epoch 1 | iter 1 step 1 | loss train: 1.428, val: n/a | iter time: 10521.57 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.627, val: n/a | iter time: 5088.83 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.435, val: n/a | iter time: 5116.08 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.330, val: n/a | iter time: 7555.83 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.287, val: n/a | iter time: 5091.70 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 1.011, val: n/a | iter time: 5092.79 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 0.962, val: n/a | iter time: 5094.40 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.834, val: n/a | iter time: 5265.32 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.879, val: n/a | iter time: 5266.57 ms (step)
Training time: 41.42s
Training time: 41.51s
Memory used: 17.73 GB
Memory used: 17.73 GB
x1000c1s5b0n0:1384744:1386102 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c1s5b0n0:1384745:1386100 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c2s0b0n1:309942:311303 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c1s5b0n0:1384746:1386099 [3] NCCL INFO [Service thread] Connection closed by localRank 3
x1000c2s0b0n1:309941:311305 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c2s0b0n1:309940:311308 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c2s0b0n1:309943:311304 [3] NCCL INFO [Service thread] Connection closed by localRank 3
x1000c1s5b0n0:1384743:1386101 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c1s5b0n0:1384746:1390991 [3] NCCL INFO comm 0x1487eea0 rank 3 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c1s5b0n0:1384745:1390992 [2] NCCL INFO comm 0x14e38430 rank 2 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c1s5b0n0:1384744:1390990 [1] NCCL INFO comm 0x14511ca0 rank 1 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c2s0b0n1:309943:316142 [3] NCCL INFO comm 0x1526c2c0 rank 7 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c2s0b0n1:309942:316140 [2] NCCL INFO comm 0x13c9a0f0 rank 6 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c2s0b0n1:309941:316139 [1] NCCL INFO comm 0x148d8f00 rank 5 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c2s0b0n1:309940:316141 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
x1000c1s5b0n0:1384743:1390993 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
