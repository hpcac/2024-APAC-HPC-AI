time mpirun -mca opal_common_ucx_opal_mem_hooks 1 -wdir /home/users/industry/ai-hpc/apacsc22/scratch/workdir/hoomd -output-filename /home/users/industry/ai-hpc/apacsc22/run/output/hoomd.nodes32.WS40000.BS80000.N200000.8345804.pbs101 -oversubscribe -map-by ppr:128:node -bind-to core -x PYTHONPATH=/home/users/industry/ai-hpc/apacsc22/scratch/workdir/hoomd/build/hoomd-openmpi-4.1.2-hpe:/home/users/industry/ai-hpc/apacsc22/scratch/workdir/hoomd/hoomd-benchmarks /home/users/industry/ai-hpc/apacsc22/scratch/workdir/hoomd/hoomd.py312/bin/python -m hoomd_benchmarks.md_pair_wca --device CPU -v -N 200000 --repeat 1 --warmup_steps 40000 --benchmark_steps 80000
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           x1002c5s6b0n1
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
[x1002c1s3b1n0:3953299] 383 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[x1002c1s3b1n0:3953299] 383 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 256 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 132 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 256 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 131 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 314 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 254 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 198 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 380 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 768 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 184 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 128 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 738 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 101 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 256 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 130 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 128 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 254 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 1 more process has sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 383 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 267 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 117 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 1 more process has sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 127 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 128 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 896 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[x1002c1s3b1n0:3953299] 549 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1002c1s3b1n0:3953299] 347 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
Using existing initial_configuration_cache/hard_sphere_200000_1.0_3.gsd
notice(2): Using domain decomposition: n_x = 16 n_y = 16 n_z = 16.
Running MDPairWCA benchmark
.. warming up for 40000 steps
.. running for 80000 steps 1 time(s)
.. 3877.4799695443335 time steps per second
3877.4799695443335
1676.78user 3964.64system 2:31.44elapsed 3724%CPU (0avgtext+0avgdata 474820maxresident)k
264inputs+64outputs (0major+2644562minor)pagefaults 0swaps
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2024-10-12 18:57:13.089831:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 8345804.pbs101
	Project: 50000022
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(4096), Used(4096)
	CPU Time Used: 01:34:01
	Memory: Requested(8192gb), Used(32236536kb)
	Vmem Used: 86752196kb
	Walltime: Requested(00:05:00), Used(00:02:34)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (x1002c1s3b1n0:ncpus=128:mem=268435456kb)+(x1002c1s3b1n1:ncpus=128:mem=268435456kb)+(x1002c1s4b0n0:ncpus=128:mem=268435456kb)+(x1002c1s4b0n1:ncpus=128:mem=268435456kb)+(x1002c1s4b1n0:ncpus=128:mem=268435456kb)+(x1002c2s0b0n1:ncpus=128:mem=268435456kb)+(x1002c2s7b0n1:ncpus=128:mem=268435456kb)+(x1002c3s1b1n0:ncpus=128:mem=268435456kb)+(x1002c3s1b1n1:ncpus=128:mem=268435456kb)+(x1002c3s2b0n0:ncpus=128:mem=268435456kb)+(x1002c3s2b1n0:ncpus=128:mem=268435456kb)+(x1002c3s2b1n1:ncpus=128:mem=268435456kb)+(x1002c3s3b0n0:ncpus=128:mem=268435456kb)+(x1002c5s6b0n1:ncpus=128:mem=268435456kb)+(x1002c7s1b0n0:ncpus=128:mem=268435456kb)+(x1002c7s6b1n0:ncpus=128:mem=268435456kb)+(x1003c0s1b0n0:ncpus=128:mem=268435456kb)+(x1003c0s1b0n1:ncpus=128:mem=268435456kb)+(x1003c0s1b1n1:ncpus=128:mem=268435456kb)+(x1003c0s2b1n1:ncpus=128:mem=268435456kb)+(x1003c1s5b1n1:ncpus=128:mem=268435456kb)+(x1003c1s6b0n0:ncpus=128:mem=268435456kb)+(x1003c1s6b0n1:ncpus=128:mem=268435456kb)+(x1003c1s6b1n1:ncpus=128:mem=268435456kb)+(x1003c1s7b0n0:ncpus=128:mem=268435456kb)+(x1003c1s7b0n1:ncpus=128:mem=268435456kb)+(x1003c1s7b1n0:ncpus=128:mem=268435456kb)+(x1003c1s7b1n1:ncpus=128:mem=268435456kb)+(x1003c2s0b0n0:ncpus=128:mem=268435456kb)+(x1003c2s0b0n1:ncpus=128:mem=268435456kb)+(x1003c2s0b1n0:ncpus=128:mem=268435456kb)+(x1003c2s0b1n1:ncpus=128:mem=268435456kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
