{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1492ae95e630>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
All GPUs are fully connected via NVLink.
Number of trainable parameters: 6,738,415,616
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.117<0>
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c1s5b0n0:1384743:1384743 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.20.5+cuda12.4
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.117<0> [1]hsn1:10.150.0.244<0> [2]bond0:10.168.0.35<0>
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Using non-device net plugin version 0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Using network Socket
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init START
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/04 :    0   3   6   5   4   7   2   1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/04 :    0   5   7   6   4   1   3   2
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/04 :    0   3   6   5   4   7   2   1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/04 :    0   5   7   6   4   1   3   2
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO P2P Chunksize set to 131072
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Connected all rings
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO Connected all trees
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c1s5b0n0:1384743:1386095 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init COMPLETE
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
Epoch 1 | iter 1 step 1 | loss train: 1.417, val: n/a | iter time: 10486.38 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.435, val: n/a | iter time: 5116.08 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.287, val: n/a | iter time: 5091.70 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 1.011, val: n/a | iter time: 5092.79 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.834, val: n/a | iter time: 5265.32 ms (step)
Training time: 41.42s
Memory used: 17.73 GB
x1000c1s5b0n0:1384743:1386101 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c1s5b0n0:1384743:1390993 [0] NCCL INFO comm 0x14c1b8c0 rank 0 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
