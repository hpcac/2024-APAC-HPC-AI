{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14ac13e51c70>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc22/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
All GPUs are fully connected via NVLink.
Number of trainable parameters: 6,738,415,616
x1000c2s0b0n1:309940:309940 [0] NCCL INFO cudaDriverVersion 12020
x1000c2s0b0n1:309940:309940 [0] NCCL INFO Bootstrap : Using hsn0:10.150.0.156<0>
x1000c2s0b0n1:309940:309940 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.0.156<0> [1]hsn1:10.150.0.160<0> [2]bond0:10.168.0.20<0>
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Using non-device net plugin version 0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Using network Socket
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init START
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c2s0b0n1:309940:311299 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/-1/-1->4->0 [2] 5/0/-1->4->-1 [3] 5/0/-1->4->-1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO P2P Chunksize set to 131072
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Connected all rings
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c2s0b0n1:309940:311299 [0] NCCL INFO Connected all trees
x1000c2s0b0n1:309940:311299 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c2s0b0n1:309940:311299 [0] NCCL INFO 4 coll channels, 0 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c2s0b0n1:309940:311299 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x59d1f031ecde0ebc - Init COMPLETE
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
Epoch 1 | iter 1 step 1 | loss train: 1.428, val: n/a | iter time: 10521.57 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.627, val: n/a | iter time: 5088.83 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.330, val: n/a | iter time: 7555.83 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 0.962, val: n/a | iter time: 5094.40 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.879, val: n/a | iter time: 5266.57 ms (step)
Training time: 41.51s
Memory used: 17.73 GB
x1000c2s0b0n1:309940:311308 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c2s0b0n1:309940:316141 [0] NCCL INFO comm 0x1455bbf0 rank 4 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
